{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CenterNet (AkidaNet18)/PASCAL-VOC detection example\n",
    "\n",
    "This example demonstrates Akida's object detection capabilities using the CenterNet (AkidaNet18) architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Import model\n",
    "\n",
    "The model used for this demonstration can be found at the [Akida 2.0 Model Zoo](https://doc.brainchipinc.com/model_zoo_performance.html#id4). It has been pre-downloaded and converted to Akida here for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from akida import Model\n",
    "\n",
    "model_akida = Model(\"models/centernet_akidanet18_voc_384_i8_w8_a8.fbz\")\n",
    "model_akida.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Map the model onto the FPGA\n",
    "\n",
    "For more details on this flow, please see: [Model Hardware Mapping](https://doc.brainchipinc.com/user_guide/akida.html#model-hardware-mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import akida\n",
    "from cnn2snn import set_akida_version, AkidaVersion\n",
    "# Instantiate akida model\n",
    "with set_akida_version(AkidaVersion.v2):\n",
    "    devices = akida.devices()\n",
    "    if len(devices) > 0:\n",
    "        print(f'Available devices: {[dev.desc for dev in devices]}')\n",
    "        device = devices[0]\n",
    "        print(device.version)\n",
    "        try:\n",
    "            model_akida.map(device)\n",
    "            print(f\"Mapping to Akida device {device.desc}.\")\n",
    "        except Exception as e:\n",
    "            print(\"Model not compatible with FPGA. Running on CPU.\")\n",
    "    else:\n",
    "        print(\"No Akida devices found, running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Model Summary After Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_akida.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3 Run inference on a image\n",
    "\n",
    "Here, an image is preprocessed and evaluated through the Akida model. Bounding boxes for detected objects are returned and superimposed on the image using the anchors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from akida_models.detection.processing import preprocess_image\n",
    "from akida_models.centernet.centernet_processing import decode_output\n",
    "\n",
    "import cv2\n",
    "\n",
    "# PASCAL VOC object detection class labels\n",
    "labels = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "         'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n",
    "         'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
    "         'train', 'tvmonitor']\n",
    "\n",
    "# Get model input dimensions\n",
    "input_shape = model_akida.layers[0].input_dims\n",
    "\n",
    "# Load and preprocess input image\n",
    "raw_image = cv2.imread(\"img/train.jpg\")\n",
    "raw_height, raw_width, _ = raw_image.shape\n",
    "\n",
    "# Preprocess image for model input\n",
    "image = preprocess_image(raw_image, input_shape)\n",
    "input_image = image[np.newaxis, :].astype(np.uint8)\n",
    "\n",
    "output_img = np.copy(raw_image)\n",
    "\n",
    "# Run object detection inference\n",
    "pots = model_akida.predict(input_image)[0]\n",
    "\n",
    "# Decode predictions to bounding boxes\n",
    "bounding_boxes = decode_output(pots,\n",
    "                       20,  # Number of classes\n",
    "                       obj_threshold=0.4,  # Confidence threshold\n",
    "                       max_detections=100,\n",
    "                       kernel=5)\n",
    "\n",
    "# Draw bounding boxes and labels on output image\n",
    "for box in bounding_boxes:\n",
    "       # Draw rectangle around detected object\n",
    "        output_img = cv2.rectangle(output_img, \n",
    "                   (int(box.x1 * raw_width), int(box.y1 * raw_height)),\n",
    "                   (int(box.x2 * raw_width), int(box.y2 * raw_height)),\n",
    "                   color=(0, 255, 0),  # Green box\n",
    "                   thickness=2)\n",
    "        label_text = labels[box.get_label()]\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        font_scale = 0.8\n",
    "        thickness = 2\n",
    "        \n",
    "        # Get text size for background rectangle\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(label_text, font, font_scale, thickness)\n",
    "        \n",
    "        # Draw background rectangle\n",
    "        text_x = int(box.x1 * raw_width)\n",
    "        text_y = int(box.y1 * raw_height - 10)\n",
    "        output_img = cv2.rectangle(output_img,\n",
    "                                  (text_x, text_y - text_height - baseline),\n",
    "                                  (text_x + text_width, text_y + baseline),\n",
    "                                  (0, 0, 0),  # Black background\n",
    "                                  -1)  # Filled rectangle\n",
    "        \n",
    "        # Draw text on background\n",
    "        output_img = cv2.putText(output_img,\n",
    "                        label_text,\n",
    "                        (text_x, text_y),\n",
    "                        font,\n",
    "                        font_scale,\n",
    "                        (255, 255, 255),  # White text\n",
    "                        thickness)\n",
    "\n",
    "  \n",
    "\n",
    "# Display results\n",
    "fig = plt.figure(num='VOC detection by Akida')\n",
    "ax = fig.subplots(1)\n",
    "img_plot = ax.imshow(np.zeros(output_img.shape, dtype=np.uint8))\n",
    "img_plot.set_data(output_img)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4 Model Statistics\n",
    "\n",
    "The advantage of running on a FPGA is the ability to accurately calculate performance, power and area metrics for a given model and process node.\n",
    "\n",
    "Here are example calculations for the CenterNet model that was tested above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "\n",
    "This Akida 6-Node FPGA is running at __25 MHz__. Based on that knowledge, we need to know the frames per second on the FPGA. The `akida.Model.statistics.fps` will return this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akida_fpga_mhz = 25  # MHz\n",
    "fps = model_akida.statistics.fps\n",
    "print(f\"FPS @ {akida_fpga_mhz} MHz: {fps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate frames per second at a different frequency use the calculation:\n",
    "\n",
    "$$ \\frac{\\text{estimated\\_fps}}{\\text{estimated\\_mhz}} = \\frac{\\text{fps}}{\\text{akida\\_fpga\\_mhz}}$$\n",
    "\n",
    "$$\\text{estimated\\_fps} = \\frac{\\text{fps} \\times \\text{estimated\\_mhz}}{\\text{akida\\_fpga\\_mhz}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_mhz = 400  # Change this to your desired frequency\n",
    "estimated_fps = (fps * estimated_mhz) / akida_fpga_mhz\n",
    "print(f\"Estimated FPS @ {estimated_mhz} MHz: {estimated_fps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Power\n",
    "\n",
    "Estimating power consumption of a model involves understanding the architecture and size of the model (e.g. # parameters, types of layer operations, etc) as well as a target process node (e.g. TSMC 28nm, Global Foundry 22nm, etc) and target chip frequency (e.g. 400 MHz, 1 GHz, etc).\n",
    "\n",
    "Brainchip's Solution Architects have access to up to date calculations for the Akida 2.0 Register-transfer-level (RTL) and can perform estimates for your particular model. Contact your BrainChip representative when you have a model ready for estimating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area\n",
    "\n",
    "Similar to power, estimating the area size required for a chip to run a model involves understanding the architecture and size of the model (e.g. # parameters, types of layer operations, etc) as well as a target process node (e.g. TSMC 28nm, Global Foundry 22nm, etc) and the ability to leverage RTL options such Hardware Partial Reconfiguration.\n",
    "\n",
    "A model that executes successfully on the FPGA is capable of receiving accurate sizing estimations. BrainChip's Solution Architects can assist with these calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5 Release the Akida FPGA device to free it up for further experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os._exit(00)"
   ]
  },
  {
   "attachments": {
    "0611b1d1-afe0-43f4-9f21-abb772e7d0fd.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can also use this button to reset your kernel)\n",
    "\n",
    "![image.png](attachment:0611b1d1-afe0-43f4-9f21-abb772e7d0fd.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
