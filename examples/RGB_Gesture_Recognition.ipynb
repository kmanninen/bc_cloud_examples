{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcee23bd",
   "metadata": {},
   "source": [
    "# RGB Gesture Recognition Example with Jesture Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a258f29",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries\n",
    "\n",
    "This cell imports essential libraries: Gradio for the interface, OpenCV for image processing, the Akida library for model execution, and NumPy and Plotly for data handling and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35785368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmanninen/miniconda3/envs/akida_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-11 15:32:59.527781: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-11 15:32:59.577568: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-11 15:32:59.577623: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-11 15:32:59.578815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-11 15:32:59.591313: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-11 15:33:01.037993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/kmanninen/miniconda3/envs/akida_env/lib/python3.11/site-packages/onnxscript/converter.py:816: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "\n",
    "from cnn2snn import set_akida_version, AkidaVersion\n",
    "import akida\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5d0b6",
   "metadata": {},
   "source": [
    "### Gauge Creation Function\n",
    "\n",
    "Defines a function using Plotly to create a gauge visualization for metrics such as frames per second during image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31006aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gauge(value):\n",
    "    fig = go.Figure(go.Indicator(\n",
    "        mode=\"gauge+number\",\n",
    "        value=value,\n",
    "        gauge={'axis': {'range': [0, 30]}},\n",
    "        domain={'x': [0, 1], 'y': [0, 1]},\n",
    "    ))\n",
    "    fig.update_layout(width=400, height=300)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944ee12",
   "metadata": {},
   "source": [
    "### Softmax Function for Arrays\n",
    "\n",
    "Implements a softmax function to convert model outputs into probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0507dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax for an array of values\n",
    "def softmaxArray(values):\n",
    "    # Assuming array shape is (1, 1, 1, x), flatten to get the values\n",
    "    values = values.ravel()\n",
    "    exp_values = np.exp(values)\n",
    "    sum_exp = np.sum(exp_values)\n",
    "    softmax_values = exp_values / sum_exp\n",
    "    return softmax_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e976a9",
   "metadata": {},
   "source": [
    "### Image Configuration and Output Decoding\n",
    "\n",
    "Sets up image parameters and label names, and includes a function to preprocess images and decode predictions into readable labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ebc6633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode numeric labels into human readable ones: contains all string names for classes\n",
    "# available in the dataset\n",
    "import os\n",
    "import csv\n",
    "# Get current working directory\n",
    "current_directory = os.getcwd()\n",
    "# Construct full file path\n",
    "file_path = os.path.join(current_directory, 'datasets/jester_subset', 'jester-v1-labels.csv')\n",
    "# Open the file\n",
    "with open(file_path, 'r') as csvfile:\n",
    "    labels = [row[0] for row in csv.reader(csvfile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48246e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_x = 100\n",
    "image_y = 100\n",
    "image_z = 3\n",
    "def decodeOutput(inp):\n",
    "        global akida_model\n",
    "        inp = cv2.resize(inp, (image_x, image_y))\n",
    "        inp = inp.reshape((-1, image_x, image_y, image_z))\n",
    "        timer_start = time.time()\n",
    "        predictions = softmaxArray(akida_model.predict(inp))\n",
    "        frame_time = time.time() - timer_start\n",
    "        fps = 1 / frame_time if frame_time > 0 else 0\n",
    "        confidences = {labels[i]: predictions[i] for i in range(len(predictions))}\n",
    "        sorted_confidences = dict(sorted(confidences.items(), key=lambda item: item[0]))\n",
    "\n",
    "        # Get top prediction and send to AFrame\n",
    "        top_gesture = max(confidences, key=confidences.get)\n",
    "        confidence = confidences[top_gesture]\n",
    "\n",
    "        gesture_key = top_gesture\n",
    "        #print(f\"gesture_key: {gesture_key}, confidence: {confidence}, fps: {fps}\")\n",
    "        gesture_prediction = \"\"\n",
    "\n",
    "        if confidence > 0.84:\n",
    "                gesture_to_key = {\n",
    "                'Sliding Two Fingers Left': 'ArrowLeft',\n",
    "                'Sliding Two Fingers Right': 'ArrowRight',\n",
    "                'Zooming In With Two Fingers': 'ArrowUp', \n",
    "                #'Zooming Out With Full Hand': 'ArrowDown',  # Commented out to avoid conflict with zooming in\n",
    "                'Shaking Hand': 'Space'\n",
    "                }\n",
    "                gesture_key = gesture_to_key.get(top_gesture, \"\")\n",
    "                gesture_prediction = top_gesture\n",
    "                #print(f\"Gesture key: {gesture_key}\")\n",
    "        else:\n",
    "                gesture_key = \"\"\n",
    "                gesture_prediction = \"\"\n",
    "                # print(\"No valid gesture detected\")\n",
    "                \n",
    "                \n",
    "\n",
    "        return sorted_confidences, fps, gesture_key, gesture_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8830e8d",
   "metadata": {},
   "source": [
    "### Image Classification Wrapper\n",
    "\n",
    "A function that processes an image, decodes it, and returns classification confidences with a gauge visualization of processing speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bccd78e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(inp):\n",
    "\n",
    "  confidences, fps, gesture_key, gesture_prediction = decodeOutput(inp)\n",
    "  #print(f\"Gesture: {gesture_key}, FPS: {fps}\")\n",
    "\n",
    "  return create_gauge(round(fps, 2)), gesture_key, gesture_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afcdd6",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model\n",
    "\n",
    "Loads a pre-trained quantized model for visual wake word detection via `akida_models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc99fd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model Summary                  \n",
      "________________________________________________\n",
      "Input shape    Output shape  Sequences  Layers\n",
      "================================================\n",
      "[100, 100, 3]  [1, 1, 27]    1          18    \n",
      "________________________________________________\n",
      "\n",
      "_________________________________________________________________________\n",
      "Layer (type)                            Output shape   Kernel shape    \n",
      "\n",
      "================== SW/input_conv-dequantizer (Software) =================\n",
      "\n",
      "input_conv (InputConv2D)                [50, 50, 8]    (3, 3, 3, 8)    \n",
      "_________________________________________________________________________\n",
      "convt_full_1_0 (BufferTempConv)         [50, 50, 20]   (1, 1, 40, 20)  \n",
      "_________________________________________________________________________\n",
      "convs_full_1_0 (Conv2D)                 [25, 25, 40]   (3, 3, 20, 40)  \n",
      "_________________________________________________________________________\n",
      "convt_full_2_0 (BufferTempConv)         [25, 25, 80]   (1, 1, 200, 80) \n",
      "_________________________________________________________________________\n",
      "convs_full_2_0 (Conv2D)                 [13, 13, 120]  (3, 3, 80, 120) \n",
      "_________________________________________________________________________\n",
      "convt_full_3_0 (BufferTempConv)         [13, 13, 160]  (1, 1, 600, 160)\n",
      "_________________________________________________________________________\n",
      "convs_full_3_0 (Conv2D)                 [7, 7, 200]    (3, 3, 160, 200)\n",
      "_________________________________________________________________________\n",
      "convt_dw_4_0 (DepthwiseBufferTempConv)  [7, 7, 200]    (1, 1, 5, 200)  \n",
      "_________________________________________________________________________\n",
      "convt_pw_4_0 (Conv2D)                   [7, 7, 240]    (1, 1, 200, 240)\n",
      "_________________________________________________________________________\n",
      "convs_dw_4_0 (DepthwiseConv2D)          [4, 4, 240]    (3, 3, 240, 1)  \n",
      "_________________________________________________________________________\n",
      "convs_pw_4_0 (Conv2D)                   [4, 4, 280]    (1, 1, 240, 280)\n",
      "_________________________________________________________________________\n",
      "convt_dw_5_0 (DepthwiseBufferTempConv)  [4, 4, 280]    (1, 1, 5, 280)  \n",
      "_________________________________________________________________________\n",
      "convt_pw_5_0 (Conv2D)                   [4, 4, 320]    (1, 1, 280, 320)\n",
      "_________________________________________________________________________\n",
      "convs_dw_5_0 (DepthwiseConv2D)          [2, 2, 320]    (3, 3, 320, 1)  \n",
      "_________________________________________________________________________\n",
      "convs_pw_5_0 (Conv2D)                   [1, 1, 640]    (1, 1, 320, 640)\n",
      "_________________________________________________________________________\n",
      "dense (Dense1D)                         [1, 1, 640]    (640, 640)      \n",
      "_________________________________________________________________________\n",
      "dense_1 (Dense1D)                       [1, 1, 27]     (640, 27)       \n",
      "_________________________________________________________________________\n",
      "dequantizer (Dequantizer)               [1, 1, 27]     N/A             \n",
      "_________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from akida_models.model_io import load_model\n",
    "akida_model = load_model(\"models/tenn_spatiotemporal_jester_buffer_i8_w8_a8.fbz\")\n",
    "akida_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0efcf",
   "metadata": {},
   "source": [
    "Map the `akida_model` onto the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2ea800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Akida devices found, running on CPU.\n"
     ]
    }
   ],
   "source": [
    "with set_akida_version(AkidaVersion.v2):\n",
    "            devices = akida.devices()\n",
    "            if len(devices) > 0:\n",
    "                print(f'Available devices: {[dev.desc for dev in devices]}')\n",
    "                device = devices[0]\n",
    "                print(device.version)\n",
    "                try:\n",
    "                    akida_model.map(device)\n",
    "                    print(f\"Mapping to Akida device {device.desc}.\")\n",
    "                    mappedDevice = device.version\n",
    "                except Exception as e:\n",
    "                    print(\"Model not compatible with FPGA. Running on CPU.\")\n",
    "                    mappedDevice = \"CPU\"\n",
    "            else:\n",
    "                print(\"No Akida devices found, running on CPU.\")\n",
    "                mappedDevice = \"CPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29462c7-a75f-479f-a260-8ddc9a6332c4",
   "metadata": {},
   "source": [
    "### Model Summary After Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddc6a3-7e8c-4f66-a896-7c93fe7fcc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "akida_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ee5a7",
   "metadata": {},
   "source": [
    "### Gradio Interface Setup\n",
    "\n",
    "Creates a Gradio interface to capture webcam images, display device information, and stream classified images using the Akida model. The interface shows live predictions and frame processing speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b0e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theme = gr.themes.Base(\n",
    "    text_size=\"sm\",\n",
    "    spacing_size=\"sm\",\n",
    "    radius_size=\"sm\",\n",
    ")\n",
    "\n",
    "gr.set_static_paths(paths=[\"vr/\", \"img/\"])\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks(\n",
    "    title=\"Brainchip\",\n",
    "    fill_width=True,\n",
    "    fill_height=True,\n",
    "    delete_cache=[180, 600],\n",
    "    theme=theme\n",
    ") as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "        <h1 style=\"text-align: center;\">Akida Cloud</h1>\n",
    "        <br>\n",
    "        \"\"\")\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"## Gesture Recognition with RGB Camera\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_img = gr.Image(sources=[\"webcam\"], type=\"numpy\")\n",
    "            gr.Markdown(\"**ℹ️ Please press the 'Record' button to start inference.**\")\n",
    "            gesture_prediction = gr.Textbox(visible=True, label=\"Gesture\", placeholder=\"Detected gesture will appear here\")\n",
    "        with gr.Column():\n",
    "            gr.Markdown(f\"\"\"Device: {mappedDevice}\"\"\")\n",
    "            js_output = gr.HTML(visible=False)\n",
    "            gesture_output = gr.Textbox(visible=False)\n",
    "            aframe_component = gr.HTML(\"\"\"\n",
    "                <iframe src=\"/gradio_api/file=vr/photo_gallery.html\" \n",
    "                        width=\"800\" height=\"600\" \n",
    "                        frameborder=\"0\"\n",
    "                        id=\"aframe-iframe\">\n",
    "                </iframe>\n",
    "            \"\"\")\n",
    "            #output_label = gr.Label(elem_classes=labels, visible=False, render=False, label=\"Classification Results\")\n",
    "            #output_dataframe = gr.DataFrame(label=\"Classification Results\", headers=[\"Label\", \"Confidence\"]) \n",
    "            #print(output_label)\n",
    "            plot = gr.Plot(label=\"Frames per second\")\n",
    "        dep = input_img.stream(classify_image, [input_img], [plot, gesture_output, gesture_prediction],\n",
    "                                time_limit=30, stream_every=0.1, concurrency_limit=30)      \n",
    "        gesture_output.change(\n",
    "            fn=None,\n",
    "            js=\"\"\"(gesture) => {\n",
    "                                    console.log('Gesture key:', gesture);\n",
    "                                    // Send gesture to AFrame iframe\n",
    "                                    if (gesture && gesture.trim() !== '') {\n",
    "                                        console.log('Sending gesture during stream:', gesture);\n",
    "                                        let iframe = document.getElementById('aframe-iframe');\n",
    "                                        if (iframe) {\n",
    "                                            iframe.contentWindow.postMessage({\n",
    "                                                type: 'gesture',\n",
    "                                                keyCode: gesture\n",
    "                                            }, '*');\n",
    "                                        }\n",
    "                                    }\n",
    "                                    return outputs;\n",
    "                                }\"\"\",\n",
    "            inputs=[gesture_output], outputs=[]\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a98800-c506-4355-8bd7-5b04d94e4fcb",
   "metadata": {},
   "source": [
    "#### Note: Once you’ve finished running the notebook, ***\"Uncomment and run the cell below\"*** to release the device and free it up for further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85274cf3-2ad3-4f67-81c3-cbd38e0fe336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os._exit(00)"
   ]
  },
  {
   "attachments": {
    "b5ba76da-de55-4da5-81ca-4519b7f3021b.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAAwCAYAAADw1toQAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAA+KSURBVHhe7d19bBR1Hsfx9+xzu/SJlj5QbEtZWyzB1gMraPRycOqJOcA7OQ44kRwafDg1kBiJonKH5kLiQ0TB5C7BBETxAQVFOHkQQ0tPLIKV0gdaWmgphaUP2273ofs09wftxh2wttCeYr+vZP6Z33emO7vbz8z85jeziqqqKkIIIX7RdNoZQgghfnkk7IUQYhiQsBdCiGFAwl4IIYYBCXtxsWAQ5Lq9EL8oEvYikqrSXVqKv6wM1ePRtgohrlKKDL0UEYJBfAcP0v3ll6DTYZ4+HVNBARiN2kohxFVEwl5cktrRgb+yEvdHH6GLjWXEkiXoRo3SlgkhrhIS9qJPqs+H99NP6Vq3DuuiRUTdcw9KdDTopAdQiKuJhL3ol1BrK5533iHY3IzxppswTZmCPiVFWyaE+JmSsBf9Fwzir6nBV1xMoLoaU2Eh5rvuQjdiBKHWVnSxsdK3L8TPlIS9GDi/n1BLC66338Z/9Cgj/vY3PB9/jOGaa7A+8oi2WgjxMyBhLy6bGgoROHoU5+rV+EpKUGJjSfjXvzDdeCPo9aCqhJxOgidPEqiuJlBfj2K1YrTZMEyYgH7UKDCbtasVQgwBCXtxRdTOTrrWrMH13nsQDGKaMoXYFSswZGfjKynBvWkT3fv3E2pvj1jOMH48ljvuIPrPf0afkRHRJoQYfBL24oqoXi+BmhpCnZ3hu26NeXl4P/uMrjffJNjQAED3mDH4k5LQdXdjbmxE39WFYjDAzTfT8Ze/0D1mjGbNYrBdf/312lliGJGwF4POs307nU89RaijA/3o0cQsW4Zl5kwwmUBVCTY10fXqq3i2bAFFwTR9OvGvvooSF6ddlRhEOhkuO6xJ2ItB5S8vp2PZMvwVFRhsNuJWr8Z0003aMgBcb76Jc80aVKcT64MPEvv889oSIcQgkV29GDSq14t3+3YCNTUoMTHErlr1g0EPEL1oEdHz5qEYjXg2b8b/zTfaEiHEIJGwF4Mm5HDg2bIF1e/HOn8+5ltv1ZZEUKKiiPrjHzHk5BByOnG+9pq2RAgxSCTsL0MoFCIYDEZMV9ob5vP58Hg8BINBbdNVw7tzJ8HmZnQpKUQ/+KC2+ZKMEyZg/vWvwWDAX1aGv6xMWyKEGARD0mevqiotLS34fL6I+Xq9nsTERIw/wV2WXV1dnDp1Cp1OR0ZGBlarFXpea3t7O6FQiKSkJO1iF2lvb+fYsWN0d3dHzM/KymLcuHER835MR0cHx48fB6CoqIhgMEh2djYZGRkYDAYKCgpQFEW72M9W6+zZ+A4dIupPfyJu1SqUnvf4x/iKi2l/+GFUj4eYJ5/EumSJtkQIcYX0K1euXKmdeaXOnz/Phx9+SG1tLcePHw9PBw4cQFEUxowZc1kjA/x+Py6XC/Nl3Ijz6aefcvz4cRoaGgiFQqSnpwNQX1/Pli1b8Pl82Gw27WIXOXHiBFVVVaSlpeF2uwmFQoRCIQwGAzqdDoPBgMFg0C52SYcOHWLfvn0cPnyYhoYG0tPTKSoqwu128/XXXzN58mSioqK0i/XJ6/USCAT6/RoGS8jpxPniixAKYV2wANOvfgU9O6rW1la+++47qqur8Xq9xMbG0tLSwogRIwDQJSbifu89Qu3t6NPSsPzmNxduyhJCDJqBJ24/tLe3o9PpmDdvXsR04sQJtm7dSllZ2WV1ezQ2NrJr1y7t7H6pqalh2rRp5Ofn09bWRjAYpLW1lQ0bNmA0GrmpjwuJ3xcMBhk5ciRNTU0cOXKE6upqqqurKSoq4t1332XHjh393jav10teXh75+flkZWXR1dVFdnY2t912G3FxcXgu48dDTp48ybZt23C73dqmAXnrrbd47bXXIqadO3dqy8ICFRWobje6hAT0Y8aEn4rZ0NDAxo0bOXv2LNHR0RQXF7N27Vp27NgRXlaxWjFedx2EQgSbmy+6AQvg22+/Db+vfr+fiooKbcmAffnllyxbtoz2nr/n9XpZt24du3bt6vMzPH36NKtXr9bOBqC5uZlNmzbhdDq1TUL8pIYk7AGcTifl5eUREz1dF01NTf3um1ZVFb/fj8/nw+v14nK58Pl8+Hy+Pv8htW6//fZwGJeVlWG329m4cSOpqanMnTuX+Ph47SJ9am5uZsaMGTz++OPh6b777qOlpYVAIKAtvyRVVYmNjWXBggWsWLGCp59+mhUrVlBYWIiiKAPavl6BQICDBw/y7LPPUlxcjNvtvqz1OJ1OysrKIqa+dj6h8+cBUEaMQOk5Yvd6vZSUlDB16lRmzZrFlClTKCwsxG63X7Su3rtoVa/3kr+QtXPnTl5++WXOnDmD3+9n/fr1rF+/nra2NkKhkLa8X4LBIG1tbdTX10PPwURxcTE6nY5AIBA+M3U4HIRCIc6cOUNDQwMOhwOHw0EwGKS5uRmPx4PL5aKqqoq6urpwt6DD4aCqqoqamhq8Xi9Op5Ouri66urpobW0N13g8Hs6dO8epU6c4efLkRV2EWmVlZdTU1ITPKuvr6ykqKtKWCRFhSLpxWlpa2Lt3L1999RWHDx8OT71yc3PJycnpV1eO2+2muLiY0tJSqqqqaGxsxG63U1FRgc1m63f//+jRo5kwYQI2m42amhqqq6uZPHky06dPJyEhQVv+g86dO4fT6cThcJCZmUnK9x7z6/F4qKys5IYbbkDfj26Iuro6dDodY8eO1TZRXFxMfn4+sbGx2qY+2e12jhw5gsPh4OjRo7S3t5OSkkJMTMyA+v8PHjzI2bNnI+bl5ORw3XXXRczr5Tt4kO69e9GnphJ1993oU1JwuVxUVlZSWFiI1WolGAxit9tJTU1l/PjxpKamhpf37tmD/9tv0SUlYfntb9ElJkas/8CBA+GzqPj4eMrLy6mtreXEiROMHj2ahISEAW0fPTvszs5ODAYDNpuNffv2YTKZSE9PJyUlhXXr1rFjxw48Hg/Z2dk8+uijnDp1CqvVSkNDAz6fj/3795OUlMT777/P1q1bKS8vx+/3M2nSJNasWcOePXsoKirC5XJx/vx5Kisrqa2tZdu2beTn57N161Z0Oh3PPPMMdXV17Nq1i5EjR5KZmfmD2+Pz+di9ezeBQAC3282+ffsYO3YsaWlp2lIhwn48ba9Q75HzpEmTwvN+6Et8KUajkaysLPLy8sjKymLkyJHk5eWRl5fX76Cn5yi6o6MDu92Oz+fD5XKxd+9e7Ha7tvQXw2g0kpaWRnR0tLZp8PXuuL93FqEoSsQZSu9F56SkpIvO7JTez7KP74aiKFitVqxWa3jdsbGxA76u0UtRFHJzc7Hb7Rw+fJjKykrGjx8PPe+dx+Ohvr6ew4cP4/V6iY+P55FHHmHixIkcOHCAiooKFi9eTGxsLHV1daxdu5Zly5aRnJxMY2MjLpeLN954g1WrVlFaWkp6ejq1tbW4XC66urooKysjJiaGhIQE4uLiWL58ObNnzw6fGfyQjIwM7rnnHsrLy/n8888pLCyURyGIHzUkYd8b5nq9noKCAu644w4WLlwYbh9It4LJZMJms1FQUIDNZiMpKYmCggIKCgoGFPZbtmzhk08+obS0lOTkZJ544gmSk5N55513OHXqlLb8qjdhwgSee+457rrrrss66h0opefsSO3uRvV6ATCbzZjNZo4fPx7+zM+dOxc+eo3QE26K2YxisUS29bjzzjtZunQp1157LSaTiXnz5vHwww+Tnp5+2duXlJRERkYGL7zwAjNnzgx/pz7++GMWLFjAK6+8Qnp6OqqqEh0dHd6x3HzzzaiqSklJCVarlbNnz9LZ2Yndbuf8+fPEx8fT2NiI2+3GbrcTCATIzMzE7XZjsViYPn06H330ETabDb1ez6hRo4iJicFkMvXr4npycjILFy7k3nvvJT8/v19nkmJ4G5Kw7+2eCQaD1NbWcuzYMXbv3h1u1+v1l/XPaTabB9Tl8n2nT59m2rRpTJ48mYyMDCwWC4sWLWLMmDF88MEHNDU1aRe5pPz8fGbMmDGgHU1f/H4/brf7oqmvI7u+REdHM3v2bB577DFSUlIu630GKCgoYO7cuRFTX0NTDT1dMmpn54WHovV8XrfeeitVVVVs3ryZDRs28PbbbzNx4sTwaKhewZ5+cyU6+sLPHmo88MADzJkzB7PZjMFg4KmnnmLatGlXFHJxcXHEx8czadIkbrnlFsaPH09iYiIJCQnk5OSwdOlSnn/+eYxGIyaTiXHjxmEwGLBYLIwbN46HHnqIkpISysvLmT9/PnPmzGHdunVkZWUxevRo7r//fmbNmsVLL73EihUrsFqt5OfnEx8fz9SpUzGbzaSlpWGxWBg7diyKopCQkEBiYmK/Pjez2XxFOzoxvAzJOPvOzk727NlDR0cHABaLBZ/PRygUQq/XU1hYSG5u7oC/pIFAgO7u7vAY+YE4duwYpaWlWCwWbrzxRrKzs1EUhba2NoqLi0lJSen3iByAzZs343A4SE5ODs/rHfa4YMGCfoXQN998w6FDhyLW0aumpoYlS5YQN8CHg/XuJPpzPWQwqS4X5yZPRnW5iPv734leuDA8fLKjo4OGhgb8fj/XXHMNiYmJEa8v1NFBy+9+R7Cpiej584l74QXox9GtEKL/hiTs6Tli1fbL0tPFYzQa//9hpKo4HI5wP+/3/77f70dVVUwmU8Qyfem9APr9bdTr9eTm5l4yvC+lu7ub9vb2S3Zr9Z7aD3SH+FNqnTMH33//S9Ts2cT9858oMTHakkvq/uILHI89hurzEbN8OdbFi7UlQogrNGRhL4Yf9+bNdDz5JLqEBJK2b+/fj5KEQnT+4x+43noLXWIiIzdtujDmXggxqP6/h9fiF80ybRqGzExCbW10vfGGtvmS/GVldO/ff+FXrgoLJeiFGCIS9mLQKPHxRM2di2Iy4dm6Fe/27dqSCKrLhfvddwnU1aGLjydm6VJtiRBikEjYi0GjmExY7rwTQ14eqttN58qVdO/bpy0L63r9dTwffgiBANa//hVDbq62RAgxSKTPXgw67xdf0Ll8OcEzZ9ClpGBdvJiou+++MKRSVQnU1+P697/x/uc/YDBgmTmT+Bdf7PcFXSHEwEnYiyHh3b6drtdfx3/sGAD6zEx0qanQ3U3gxAlUpxPFZCJq1ixGPPEE+qws7SqEEINIwl4MGX95OZ7Nm/F89ln4QWm9DAUFRP/hD0T9/vfoRo2KaBNCDD4JezGk1GAQta0Nf3U1wfp6lBEjMObkoLfZUEymPp+FI4QYPBL2QggxDMhoHCGEGAYk7IUQYhiQsBdCiGFAwl4IIYYBCXshhBgGJOyFEGIYkLAXQohhQMJeCCGGgf8BIp2xzavfdkcAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "c4fd993f-bdf5-44e6-92d0-cc04a371931c",
   "metadata": {},
   "source": [
    "#### You can also use this button to reset your kernel\n",
    "\n",
    "![image.png](attachment:b5ba76da-de55-4da5-81ca-4519b7f3021b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53cb95-ef38-4d4d-b8e8-8a7cbf3b9fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "akida_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
