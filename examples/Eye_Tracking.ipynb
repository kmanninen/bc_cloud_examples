{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d5882-80f7-4f77-aa7b-19ca6ffb1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0e99e",
   "metadata": {},
   "source": [
    "# Eye Tracking Example\n",
    "\n",
    "This example demonstrates Akida's eye tracking capabilities using the Brainchip's spatiotemporal architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb23b79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Import model\n",
    "\n",
    "The model used for this demonstration can be found at the [Akida 2.0 Model Zoo](https://doc.brainchipinc.com/model_zoo_performance.html#eye-tracking). It has been pre-downloaded and converted to Akida here for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d17fb-2906-46e0-afb8-e26a7e784ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from akida import Model\n",
    "\n",
    "model = Model(\"models/tenn_spatiotemporal_eye_buffer_i8_w8_a8.fbz\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf653de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Map the model onto the FPGA\n",
    "\n",
    "For more details on this flow, please see: [Model Hardware Mapping](https://doc.brainchipinc.com/user_guide/akida.html#model-hardware-mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import akida\n",
    "from cnn2snn import set_akida_version, AkidaVersion\n",
    "# Instantiate akida model\n",
    "with set_akida_version(AkidaVersion.v2):\n",
    "    devices = akida.devices()\n",
    "    if len(devices) > 0:\n",
    "        print(f'Available devices: {[dev.desc for dev in devices]}')\n",
    "        device = devices[0]\n",
    "        print(device.version)\n",
    "        try:\n",
    "            model.map(device)\n",
    "            print(f\"Mapping to Akida device {device.desc}.\")\n",
    "            mappedDevice = device.version\n",
    "        except Exception as e:\n",
    "            print(\"Model not compatible with FPGA. Running on CPU.\")\n",
    "            mappedDevice = \"CPU\"\n",
    "    else:\n",
    "        print(\"No Akida devices found, running on CPU.\")\n",
    "        mappedDevice = \"CPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db70ab1-31d6-47a2-b865-28acdda45b39",
   "metadata": {},
   "source": [
    "#### Model Summary After Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521a34f-db06-47a1-8f0d-97942b947cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895133a4-806d-4c0b-bbe8-7431ae1b6325",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3 Run inference on a sample batch of events\n",
    "\n",
    "Here, sample events from a DVS camera are preprocessed and evaluated through the Akida model. Event coordinates are visualized, including the tracking prediction location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d741fc2",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries\n",
    "\n",
    "This cell imports essential libraries: matplotplib for event visualizations, the Akida libraries for model execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c43cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# Akida neural processor imports\n",
    "from cnn2snn import set_akida_version, AkidaVersion\n",
    "import akida\n",
    "\n",
    "# Eye tracking model preprocessing and postprocessing utilities\n",
    "from akida_models.tenn_spatiotemporal.eye_preprocessing import preprocess_data\n",
    "from akida_models.tenn_spatiotemporal.eye_losses import process_detector_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a72c1f",
   "metadata": {},
   "source": [
    "####  Load and segment the sample events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe2773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_segment_npy(file_path, time_window_us=10_000, segment_duration_us=500_000):\n",
    "   \"\"\"\n",
    "   Loads event data from structured .npy file and segments into 500ms windows\n",
    "   for model processing.\n",
    "   \n",
    "   Args:\n",
    "       file_path (str): Path to event file with 'p', 'x', 'y', 't' fields\n",
    "       time_window_us (int): Frame time window in microseconds (10ms default)\n",
    "       segment_duration_us (int): Segment duration in microseconds (500ms default)\n",
    "   \n",
    "   Returns:\n",
    "       tuple: (processed frame tensors list, raw event segments list)\n",
    "   \"\"\"\n",
    "   # Load structured event data\n",
    "   data = np.load(file_path)\n",
    "\n",
    "   # Extract and convert event fields to float32\n",
    "   p = data['p'].astype('float32')  # Polarity\n",
    "   x = data['x'].astype('float32')  # X coordinate\n",
    "   y = data['y'].astype('float32')  # Y coordinate\n",
    "   t = data['t'].astype('float32')  # Timestamp\n",
    "\n",
    "   # Stack into tensor format (4, N) for processing\n",
    "   trial = tf.stack([p, x, y, t], axis=0)\n",
    "\n",
    "   # Get time bounds for segmentation\n",
    "   t_start = t[0]\n",
    "   t_end = t[-1]\n",
    "\n",
    "   frames_list = []\n",
    "   segment_list = []\n",
    "\n",
    "   # Process data in 500ms sliding windows\n",
    "   current_time = t_start\n",
    "   while current_time + segment_duration_us <= t_end:\n",
    "       # Find event indices for current time window\n",
    "       start_idx = np.searchsorted(t, current_time, side='left')\n",
    "       end_idx = np.searchsorted(t, current_time + segment_duration_us, side='right')\n",
    "\n",
    "       # Extract event segment for current window\n",
    "       segment = tf.stack([\n",
    "           p[start_idx:end_idx],\n",
    "           x[start_idx:end_idx],\n",
    "           y[start_idx:end_idx],\n",
    "           t[start_idx:end_idx]\n",
    "       ], axis=0)\n",
    "\n",
    "       # Create dummy center label for preprocessing\n",
    "       label = tf.convert_to_tensor([[0.5, 0.5, 0]], dtype=tf.float32)\n",
    "\n",
    "       # Convert event segment to model-compatible frames\n",
    "       frames, _ = preprocess_data(\n",
    "           events=segment,\n",
    "           label=label,\n",
    "           train_mode=False,\n",
    "           frames_per_segment=1,\n",
    "           spatial_downsample=(6, 6),\n",
    "           time_window=time_window_us\n",
    "       )\n",
    "\n",
    "       frames_list.append(frames)\n",
    "       segment_list.append(segment)\n",
    "       current_time += segment_duration_us\n",
    "\n",
    "   print(f\"Processed {len(frames_list)} segments of 500ms each.\")\n",
    "   return frames_list, segment_list\n",
    "\n",
    "# Load and process eye tracking event data\n",
    "frames_all, segment_all = load_and_segment_npy(\"datasets/eye_tracking_event_examples.npy\")\n",
    "n_frames = len(frames_all)\n",
    "N, H, W, n_ch = frames_all[0].shape\n",
    "\n",
    "print(f\"Loaded data with {n_frames} frames, {H}Ã—{W} pixels, {n_ch} channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0b442",
   "metadata": {},
   "source": [
    "#### Perform eye tracking inference against sample event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e207083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up color mapping for visualization channels\n",
    "if n_ch == 2:\n",
    "   # Two-channel mode: red and blue\n",
    "   colors = np.array([[255, 0, 0], [0, 0, 255]], dtype=np.uint8)\n",
    "else:\n",
    "   # Multi-channel mode: use matplotlib's tab10 color palette\n",
    "   import matplotlib\n",
    "   cmap = matplotlib.cm.get_cmap('tab10', n_ch)\n",
    "   colors = (cmap(range(n_ch))[:, :3] * 255).astype(np.uint8)\n",
    "\n",
    "# Initialize plot for real-time visualization\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "frame_number = 0\n",
    "cross_size = 3  # Size of prediction cross marker\n",
    "\n",
    "# Process and visualize each frame\n",
    "for f in frames_all:\n",
    "   frame_number += 1\n",
    "   # Start with gray background for visualization\n",
    "   frame_vis = np.full((H, W, 3), 128, dtype=np.uint8)\n",
    "\n",
    "   # Ensure frame is numpy array\n",
    "   f_np = f.numpy() if isinstance(f, tf.Tensor) else f\n",
    "   frame = f_np[0]\n",
    "\n",
    "   # Run model prediction and process output\n",
    "   pred = model.predict(f_np)\n",
    "   pred = process_detector_prediction(tf.expand_dims(pred, 0))\n",
    "\n",
    "   # Convert normalized predictions to pixel coordinates\n",
    "   y_pred_x = pred[:, 1] * W\n",
    "   y_pred_y = pred[:, 0] * H\n",
    "\n",
    "   # Extract prediction center point\n",
    "   cx = int(y_pred_x.numpy().flatten()[0])\n",
    "   cy = int(y_pred_y.numpy().flatten()[0])\n",
    "\n",
    "   # Overlay each channel's events and prediction cross\n",
    "   for ch in range(n_ch):\n",
    "       # Create mask for active events in this channel\n",
    "       mask = frame[:, :, ch] > 0\n",
    "\n",
    "       # Create cross-shaped mask at predicted location\n",
    "       pred_mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=bool)\n",
    "       for i in range(-cross_size, cross_size + 1):\n",
    "           if 0 <= cx + i < frame.shape[0]:\n",
    "               pred_mask[cx + i, cy] = True\n",
    "           if 0 <= cy + i < frame.shape[1]:\n",
    "               pred_mask[cx, cy + i] = True\n",
    "\n",
    "       # Apply colors: channel events and yellow prediction cross\n",
    "       frame_vis[mask] = colors[ch]\n",
    "       frame_vis[pred_mask] = [255, 255, 0]  # Yellow cross\n",
    "   \n",
    "   # Update display with current frame\n",
    "   ax.clear()\n",
    "   ax.imshow(frame_vis)\n",
    "   ax.set_title(f'Frame {frame_number}/{n_frames}')\n",
    "   ax.axis('off')\n",
    "   \n",
    "   # Refresh display for animation effect\n",
    "   clear_output(wait=True)\n",
    "   display(fig)\n",
    "   time.sleep(0.01)  # Control playback speed\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd77db0-699b-4244-9414-51f366ac452c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4 Model Statistics\n",
    "\n",
    "The advantage of running on a FPGA is the ability to accurately calculate performance, power and area metrics for a given model and process node.\n",
    "\n",
    "Here are example calculations for the CenterNet model that was tested above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81160b51-55ff-4e99-b29b-e3b1f5caac93",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "\n",
    "This Akida 6-Node FPGA is running at __25 MHz__. Based on that knowledge, we need to know the frames per second on the FPGA. The `akida.Model.statistics.fps` will return this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724d618-ecf8-48e4-9494-927c25dab014",
   "metadata": {},
   "outputs": [],
   "source": [
    "akida_fpga_mhz = 25  # MHz\n",
    "fps = model.statistics.fps\n",
    "print(f\"FPS @ {akida_fpga_mhz} MHz: {fps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672671d1-465c-4ead-b41e-12c7264e2cde",
   "metadata": {},
   "source": [
    "To estimate frames per second at a different frequency use the calculation:\n",
    "\n",
    "$$ \\frac{\\text{estimated\\_fps}}{\\text{estimated\\_mhz}} = \\frac{\\text{fps}}{\\text{akida\\_fpga\\_mhz}}$$\n",
    "\n",
    "$$\\text{estimated\\_fps} = \\frac{\\text{fps} \\times \\text{estimated\\_mhz}}{\\text{akida\\_fpga\\_mhz}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea6f1a-d318-4067-8667-a1d6ad4d9227",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_mhz = 400  # Change this to your desired frequency\n",
    "estimated_fps = (fps * estimated_mhz) / akida_fpga_mhz\n",
    "print(f\"Estimated FPS @ {estimated_mhz} MHz: {estimated_fps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390bc160-307d-4b35-af24-fbd0def5d548",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Power\n",
    "\n",
    "Estimating power consumption of a model involves understanding the architecture and size of the model (e.g. # parameters, types of layer operations, etc) as well as a target process node (e.g. TSMC 28nm, Global Foundry 22nm, etc) and target chip frequency (e.g. 400 MHz, 1 GHz, etc).\n",
    "\n",
    "Brainchip's Solution Architects have access to up to date calculations for the Akida 2.0 Register-transfer-level (RTL) and can perform estimates for your particular model. Contact your BrainChip representative when you have a model ready for estimating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c0e08-2358-426d-8899-79719ac63724",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Area\n",
    "\n",
    "Similar to power, estimating the area size required for a chip to run a model involves understanding the architecture and size of the model (e.g. # parameters, types of layer operations, etc) as well as a target process node (e.g. TSMC 28nm, Global Foundry 22nm, etc) and the ability to leverage RTL options such Hardware Partial Reconfiguration.\n",
    "\n",
    "A model that executes successfully on the FPGA is capable of receiving accurate sizing estimations. BrainChip's Solution Architects can assist with these calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae2a2f-ad2d-4234-9b56-6f3886803080",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5 Release the Akida FPGA device to free it up for further experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e723d-7ca8-423a-823c-81f693a98ab1",
   "metadata": {},
   "source": [
    "Uncomment the code in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225220c-9853-4387-a3e6-a9a4a6ca194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os._exit(00)"
   ]
  },
  {
   "attachments": {
    "aca1db8c-68fe-46d4-8375-0d9b02509cf2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "05cbd978-dbe0-4eb8-b71b-98ec2fc60285",
   "metadata": {},
   "source": [
    "(You can also use this button to reset your kernel)\n",
    "\n",
    "![download.png](attachment:aca1db8c-68fe-46d4-8375-0d9b02509cf2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
